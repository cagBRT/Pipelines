{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2 Pipelines.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyP1CYdQ9JxmdT4aXtGgYyLz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/Pipelines/blob/main/2_Pipelines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pipelines**"
      ],
      "metadata": {
        "id": "n807p0LSnAJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A machine learning pipeline can be created by putting together a sequence of steps involved in training a machine learning model. It can be used to automate a machine learning workflow. The pipeline can involve pre-processing, feature selection, classification/regression, and post-processing. More complex applications may need to fit in other necessary steps within this pipeline.\n",
        "\n"
      ],
      "metadata": {
        "id": "_bYG4Li0m-fT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set up the Pipeline**"
      ],
      "metadata": {
        "id": "nxhavh-Unn_l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFm5lonQIpqL"
      },
      "outputs": [],
      "source": [
        "from pandas import read_csv # For dataframes\n",
        "from pandas import DataFrame # For dataframes\n",
        "from numpy import ravel # For matrices\n",
        "import matplotlib.pyplot as plt # For plotting data\n",
        "import seaborn as sns # For plotting data\n",
        "from sklearn.model_selection import train_test_split # For train/test splits\n",
        "from sklearn.neighbors import KNeighborsClassifier # The k-nearest neighbor classifier\n",
        "from sklearn.feature_selection import VarianceThreshold # Feature selector\n",
        "from sklearn.pipeline import Pipeline # For setting up pipeline\n",
        "# Various pre-processing steps\n",
        "from sklearn.preprocessing import Normalizer, StandardScaler, MinMaxScaler, PowerTransformer, MaxAbsScaler, LabelEncoder\n",
        "from sklearn.model_selection import GridSearchCV # For optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EColi Dataset Attribute Information**<br>\n",
        "\n",
        "- Sequence Name: Accession number for the SWISS-PROT database<br>\n",
        "- mcg: McGeoch's method for signal sequence recognition.<br>\n",
        "- gvh: von Heijne's method for signal sequence recognition.<br>\n",
        "- lip: von Heijne's Signal Peptidase II consensus sequence score.\n",
        "Binary attribute.<br>\n",
        "- chg: Presence of charge on N-terminus of predicted lipoproteins.\n",
        "Binary attribute.<br>\n",
        "- aac: score of discriminant analysis of the amino acid content of\n",
        "outer membrane and periplasmic proteins.<br>\n",
        "- alm1: score of the ALOM membrane spanning region prediction program.<br>\n",
        "- alm2: score of ALOM program after excluding putative cleavable signal\n",
        "regions from the sequence.<br><br>\n",
        "- label\n",
        "---\n",
        "\n",
        "\n",
        "Missing Attribute Values: None.<br>\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Class Distribution**. The class is the localization site. Please see Nakai &\n",
        "Kanehisa referenced above for more details.<br>\n",
        "\n",
        "cp (cytoplasm) 143<br>\n",
        "im (inner membrane without signal sequence) 77<br>\n",
        "pp (perisplasm) 52<br>\n",
        "imU (inner membrane, uncleavable signal sequence) 35<br>\n",
        "om (outer membrane) 20<br>\n",
        "omL (outer membrane lipoprotein) 5<br>\n",
        "imL (inner membrane lipoprotein) 2<br>\n",
        "imS (inner membrane, cleavable signal sequence) 2<br>"
      ],
      "metadata": {
        "id": "sMds6DSSoP7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read ecoli dataset from the UCI ML Repository and store in\n",
        "# dataframe df\n",
        "df = read_csv(\n",
        "    'https://archive.ics.uci.edu/ml/machine-learning-databases/ecoli/ecoli.data',\n",
        "    sep = '\\s+',\n",
        "    header=None)\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "JuGDmC8ZnzPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The data matrix X\n",
        "X = df.iloc[:,1:-1]\n",
        "# The labels\n",
        "y = (df.iloc[:,-1:])"
      ],
      "metadata": {
        "id": "Jo0fQutApeRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the labels into unique integers\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(ravel(y))\n",
        "y"
      ],
      "metadata": {
        "id": "3DWZBSntphcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into test and train\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X,  \n",
        "    y, \n",
        "    test_size=1/3,\n",
        "    random_state=0)\n",
        " \n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "id": "maB1Q7bmpWPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using a classifier without a Pipeline and Optimization**"
      ],
      "metadata": {
        "id": "rFmEJr5qprCG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We should keep in mind that the true judge of a classifier’s performance is the test set score and not the training set score. The test set score reflects the generalization ability of a classifier."
      ],
      "metadata": {
        "id": "Vm6kllnrp6eU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "knn = KNeighborsClassifier().fit(X_train, y_train)\n",
        "print('Training set score: ' + str(knn.score(X_train,y_train)))\n",
        "print('Test set score: ' + str(knn.score(X_test,y_test)))"
      ],
      "metadata": {
        "id": "4BXPkotZpnwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using a Pipeline"
      ],
      "metadata": {
        "id": "3eOhgrkxp8_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = Pipeline([\n",
        "('scaler', StandardScaler()),\n",
        "('selector', VarianceThreshold()),\n",
        "('classifier', KNeighborsClassifier())\n",
        "])"
      ],
      "metadata": {
        "id": "lKS1ml97p7un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe.fit(X_train, y_train)\n",
        "print('Training set score: ' + str(pipe.score(X_train,y_train)))\n",
        "print('Test set score: ' + str(pipe.score(X_test,y_test)))"
      ],
      "metadata": {
        "id": "RGoyI21MqEQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So it looks like the performance of this pipeline is worse than the single classifier performance on raw data. Not only did we add extra processing, but it was all in vain. Don’t despair, the real benefit of the pipeline comes from its tuning. The next section explains how to do that."
      ],
      "metadata": {
        "id": "1ytuqCLBqM7m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The parameters variable below is a dictionary that specifies the key:value pairs. Note the key must be written, with a double underscore __ separating the module name that we selected in the Pipeline() and its parameter. Note the following:\n",
        "\n"
      ],
      "metadata": {
        "id": "lTgVX-mKqqJN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The scaler has no double underscore, as we have specified a list of objects there.\n",
        "We would search for the best threshold for the selector, i.e., VarianceThreshold(). Hence we have specified a list of values [0, 0.0001, 0.001, 0.5] to choose from.\n",
        "Different values are specified for the n_neighbors, p and leaf_size parameters of the KNeighborsClassifier()."
      ],
      "metadata": {
        "id": "MK0v_jfBqzK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = {'scaler': [StandardScaler(), MinMaxScaler(),\n",
        "\tNormalizer(), MaxAbsScaler()],\n",
        "\t'selector__threshold': [0, 0.001, 0.01],\n",
        "\t'classifier__n_neighbors': [1, 3, 5, 7, 10],\n",
        "\t'classifier__p': [1, 2],\n",
        "\t'classifier__leaf_size': [1, 5, 10, 15]\n",
        "}"
      ],
      "metadata": {
        "id": "aA0qy4FVqNs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid = GridSearchCV(pipe, parameters, cv=2).fit(X_train, y_train)\n",
        "print('Training set score: ' + str(grid.score(X_train, y_train)))\n",
        "print('Test set score: ' + str(grid.score(X_test, y_test)))"
      ],
      "metadata": {
        "id": "RpcAs_-wq0Uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Don’t worry too much about the warning that you get by running the code above. It is generated because we have very few training samples and the cross-validation object does not have enough samples for a class for one of its folds."
      ],
      "metadata": {
        "id": "2xDCf4pgrVYF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By tuning the pipeline, we achieved quite an improvement over a simple classifier and a non-optimized pipeline. It is important to analyze the results of the optimization process."
      ],
      "metadata": {
        "id": "fGe1HUssrLbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the best set of parameters\n",
        "best_params = grid.best_params_\n",
        "print(best_params)\n",
        "# Stores the optimum model in best_pipe\n",
        "best_pipe = grid.best_estimator_\n",
        "print(best_pipe)"
      ],
      "metadata": {
        "id": "IaS67K6gsBFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another useful technique for analyzing the results is to construct a DataFrame from the grid.cv_results_. Let’s view the columns of this data frame."
      ],
      "metadata": {
        "id": "OkjWymausQKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_df = DataFrame.from_dict(grid.cv_results_, orient='columns')\n",
        "print(result_df.columns)"
      ],
      "metadata": {
        "id": "IlIh8bqtsGlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This DataFrame is very valuable as it shows us the scores for different parameters. The column with the mean_test_score is the average of the scores on the test set for all the folds during cross-validation. The DataFrame may be too big to visualize manually, hence, it is always a good idea to plot the results. Let’s see how n_neighbors affect the performance for different scalers and for different values of p."
      ],
      "metadata": {
        "id": "4WG2H2AIsTYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.relplot(data=result_df,\n",
        "\tkind='line',\n",
        "\tx='param_classifier__n_neighbors',\n",
        "\ty='mean_test_score',\n",
        "\thue='param_scaler',\n",
        "\tcol='param_classifier__p')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "euIo43z8sYWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plots clearly show that using StandardScaler(), with n_neighbors=7 and p=2, gives the best result. Let’s make one more set of plots with leaf_size."
      ],
      "metadata": {
        "id": "cCfmvCYTsibP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.relplot(data=result_df,\n",
        "            kind='line',\n",
        "            x='param_classifier__n_neighbors',\n",
        "            y='mean_test_score',\n",
        "            hue='param_scaler',\n",
        "            col='param_classifier__leaf_size')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9qUjCcGMsofh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}